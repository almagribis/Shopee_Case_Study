{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b35f237",
   "metadata": {},
   "source": [
    "# Different splitting small and large files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e27078",
   "metadata": {},
   "source": [
    "## Small Dataset\n",
    "- Loading the `customer-100000.csv` file at once using `pd.read_csv` consumed 75.3 MB of memory and can reduced to 69.68 MB by converting `Subscription Date` columnt to datetime objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c4b0a",
   "metadata": {},
   "source": [
    "## Large Dataset\n",
    "- Large data can also load at once but will consume far higher memory.\n",
    "- Load using chunk will reduce running memory usage, with chunksize 100000 it consume the same amount of memory with the small dataset.\n",
    "- Chunk approach will requires additional handling for task such as aggregation abd combining result across chunks.\n",
    "- For Large dataset, using other modules like `dask` or  `polar` can provide better scalability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shopee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
